{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Narcissa18/CreateAccountFormAutomation/blob/master/High_Code_SVHN_Digit_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q91KqmCRu64D"
      },
      "source": [
        "# **Deep Learning Project: Street View Housing Number Digit Recognition**\n",
        "\n",
        "# **Marks: 60**\n",
        "\n",
        "--------------\n",
        "## **Context**\n",
        "--------------\n",
        "\n",
        "One of the most interesting tasks in deep learning is to recognize objects in natural scenes. The ability to process visual information using machine learning algorithms can be very useful as demonstrated in various applications.\n",
        "\n",
        "The SVHN dataset contains over 600,000 labeled digits cropped from street-level photos. It is one of the most popular image recognition datasets. It has been used in neural networks created by Google to improve the map quality by automatically transcribing the address numbers from a patch of pixels. The transcribed number with a known street address helps pinpoint the location of the building it represents.\n",
        "\n",
        "----------------\n",
        "## **Objective**\n",
        "----------------\n",
        "\n",
        "Our objective is to predict the number depicted inside the image by using Artificial or Fully Connected Feed Forward Neural Networks and Convolutional Neural Networks. We will go through various models of each and finally select the one that is giving us the best performance.\n",
        "\n",
        "-------------\n",
        "## **Dataset**\n",
        "-------------\n",
        "Here, we will use a subset of the original data to save some computation time. The dataset is provided as a .h5 file. The basic preprocessing steps have been applied on the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8z2Z7-OAs8QG"
      },
      "source": [
        "## **Mount the drive**\n",
        "\n",
        "Let us start by mounting the Google drive. You can run the below cell to mount the Google drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "03lDyQUuef7z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfa211fa-58b3-4699-839d-aff1daa11076"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8U3DUa3eNsT"
      },
      "source": [
        "## **Importing the necessary libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-dVzeuF3eQx1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install h5py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIuA1aldjYwX",
        "outputId": "8b115eaf-9f03-465f-fbc2-1ab421b47b7a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.9.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from h5py) (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py"
      ],
      "metadata": {
        "id": "PALjvS0PjkO5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucnevGLoyKf_"
      },
      "source": [
        "**Let us check the version of tensorflow.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "W5as47YxyJVk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7b2d932c-68b7-4b40-fbb8-49745d99517b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.15.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lsux2ZwyTTR"
      },
      "source": [
        "## **Load the dataset**\n",
        "\n",
        "- Let us now load the dataset that is available as a .h5 file.\n",
        "- Split the data into the train and the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BApX9qgNsqV0",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6169f8c-7626-4343-b558-4741bac91702"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys in HDF5 file: ['X_test', 'X_train', 'X_val', 'y_test', 'y_train', 'y_val']\n",
            "Dataset: X_test\n",
            "Dataset: X_train\n",
            "Dataset: X_val\n",
            "Dataset: y_test\n",
            "Dataset: y_train\n",
            "Dataset: y_val\n",
            "Dataset shape: (42000, 32, 32)\n",
            "Dataset type: float32\n",
            "Example data points: [[[ 33.0704  30.2601  26.852  ...  71.4471  58.2204  42.9939]\n",
            "  [ 25.2283  25.5533  29.9765 ... 113.0209 103.3639  84.2949]\n",
            "  [ 26.2775  22.6137  40.4763 ... 113.3028 121.775  115.4228]\n",
            "  ...\n",
            "  [ 28.5502  36.212   45.0801 ...  24.1359  25.0927  26.0603]\n",
            "  [ 38.4352  26.4733  23.2717 ...  28.1094  29.4683  30.0661]\n",
            "  [ 50.2984  26.0773  24.0389 ...  49.6682  50.853   53.0377]]\n",
            "\n",
            " [[ 86.9591  87.0685  88.3735 ...  91.8014  89.7477  92.5302]\n",
            "  [ 86.688   86.9114  87.4337 ...  90.7306  87.204   88.5629]\n",
            "  [ 85.9654  85.8145  85.9239 ...  63.8626  59.8199  54.8805]\n",
            "  ...\n",
            "  [ 90.2236  91.0448  93.4637 ...  55.3535  48.5822  44.0557]\n",
            "  [ 90.6427  90.4039  90.937  ...  78.2696  77.4977  74.27  ]\n",
            "  [ 88.0236  88.1977  86.6709 ...  75.2206  76.6396  79.2865]]\n",
            "\n",
            " [[123.125  125.8581 122.0757 ... 123.5747 124.1186 123.3144]\n",
            "  [121.1683 124.1294 117.4613 ... 115.6078 119.5751 122.8306]\n",
            "  [124.6132 121.1019 109.6623 ... 111.1783 119.7923 124.7595]\n",
            "  ...\n",
            "  [135.1391 127.3679 117.754  ...  95.0919 105.5917 114.9283]\n",
            "  [134.8402 131.9545 124.0415 ...  93.864  105.3036 115.1132]\n",
            "  [134.8402 132.0685 128.34   ...  93.9349 104.7875 113.8252]]\n",
            "\n",
            " [[147.6196 139.6204 142.6201 ... 151.1631 151.8641 148.8644]\n",
            "  [146.6197 137.6206 140.9084 ... 151.1631 149.8643 150.8642]\n",
            "  [145.6198 136.9088 139.9085 ... 151.8641 150.8642 151.8641]\n",
            "  ...\n",
            "  [106.1785  99.2393 101.5981 ... 157.9021 156.8914 156.8914]\n",
            "  [111.879  112.238  113.0099 ... 157.1302 156.8914 157.1795]\n",
            "  [122.8779 127.2365 128.2364 ... 157.1194 157.1795 157.1795]]\n",
            "\n",
            " [[153.989  155.1783 157.4276 ... 102.065  117.9171 129.0578]\n",
            "  [160.1024 159.694  157.6556 ...  98.7664 114.2056 122.4714]\n",
            "  [161.6292 160.3349 154.9979 ...  96.2828 108.4943 115.7602]\n",
            "  ...\n",
            "  [169.6993 165.4978 154.4603 ...  54.1544  58.2033  65.3767]\n",
            "  [172.699  168.4975 158.047  ...  46.867   51.6169  61.1491]\n",
            "  [176.6986 172.4971 160.6877 ...  45.693   48.6172  58.1494]]]\n"
          ]
        }
      ],
      "source": [
        "# File path in Google Drive\n",
        "file_path = '/content/drive/MyDrive/Python Course/SVHN_single_grey1.h5'\n",
        "\n",
        "# Open the HDF5 file\n",
        "with h5py.File(file_path, 'r') as hdf:\n",
        "    # List all groups in the HDF5 file\n",
        "    print(\"Keys in HDF5 file:\", list(hdf.keys()))\n",
        "\n",
        "    # Check for the correct dataset name\n",
        "    for key in hdf.keys():\n",
        "        print(\"Dataset:\", key)\n",
        "\n",
        "    # Access a specific dataset\n",
        "    dataset_name = 'X_train'  # Replace with the actual dataset name\n",
        "    dataset = hdf[dataset_name][:]\n",
        "\n",
        "    # Print dataset information\n",
        "    print(\"Dataset shape:\", dataset.shape)\n",
        "    print(\"Dataset type:\", dataset.dtype)\n",
        "\n",
        "    # Print some data points\n",
        "    print(\"Example data points:\", dataset[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVe0CYpUgj7w"
      },
      "source": [
        "Check the number of images in the training and the testing dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3lwKpOefkpA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akTUOfLlgwoM"
      },
      "source": [
        "**Observation:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxODV6HKykuc"
      },
      "source": [
        "## **Visualizing images**\n",
        "\n",
        "- Use X_train to visualize the first 10 images.\n",
        "- Use Y_train to print the first 10 labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bvsc8ytHsqWD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzoyeXHOy80N"
      },
      "source": [
        "## **Data preparation**\n",
        "\n",
        "- Print the shape and the array of pixels for the first image in the training dataset.\n",
        "- Normalize the train and the test dataset by dividing by 255.\n",
        "- Print the new shapes of the train and the test dataset.\n",
        "- One-hot encode the target variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqndzQXng9rL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4CQkKtQ0XII"
      },
      "source": [
        "### **Normalize the train and the test data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_yUUTp_mUzB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSlYN6pb8kMY"
      },
      "source": [
        "Print the shapes of Training and Test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7FSqOpamWkH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uLxXBpz81vk"
      },
      "source": [
        "### **One-hot encode output**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zL0lYER4sqWw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViqPOTquCF76"
      },
      "source": [
        "**Observation:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yH-gVrzuByNA"
      },
      "source": [
        "## **Model Building**\n",
        "\n",
        "Now that we have done the data preprocessing, let's build an ANN model.\n",
        "\n",
        "### Fix the seed for random number generators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcKRwrGn0XIL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJDUoaEj1d6e"
      },
      "source": [
        "### **Model Architecture**\n",
        "- Write a function that returns a sequential model with the following architecture:\n",
        " - First hidden layer with **64 nodes and the relu activation** and the **input shape = (1024, )**\n",
        " - Second hidden layer with **32 nodes and the relu activation**\n",
        " - Output layer with **activation as 'softmax' and number of nodes equal to the number of classes, i.e., 10**\n",
        " - Compile the model with the **loss equal to categorical_crossentropy, optimizer equal to Adam(learning_rate = 0.001), and metric equal to 'accuracy'**. Do not fit the model here, just return the compiled model.\n",
        "- Call the nn_model_1 function and store the model in a new variable.\n",
        "- Print the summary of the model.\n",
        "- Fit on the train data with a **validation split of 0.2, batch size = 128, verbose = 1, and epochs = 20**. Store the model building history to use later for visualization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A48z6ucF0XIP"
      },
      "source": [
        "### **Build and train an ANN model as per the above mentioned architecture.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cmi81Gr5sqW-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeF8XSWz0XIU"
      },
      "source": [
        "### **Plot the Training and Validation Accuracies and write down your Observations.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lt77zgGMP4yw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGBbQpLONX7k"
      },
      "source": [
        "**Observations:_______**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0qgLMBZm5-K"
      },
      "source": [
        "Let's build one more model with higher complexity and see if we can improve the performance of the model.\n",
        "\n",
        "First, we need to clear the previous model's history from the Keras backend. Also, let's fix the seed again after clearing the backend."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_ih3wEU9wIk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lT6o3TIKuCtk"
      },
      "source": [
        "### **Second Model Architecture**\n",
        "- Write a function that returns a sequential model with the following architecture:\n",
        " - First hidden layer with **256 nodes and the relu activation** and the **input shape = (1024, )**\n",
        " - Second hidden layer with **128 nodes and the relu activation**\n",
        " - Add the **Dropout layer with the rate equal to 0.2**\n",
        " - Third hidden layer with **64 nodes and the relu activation**\n",
        " - Fourth hidden layer with **64 nodes and the relu activation**\n",
        " - Fifth hidden layer with **32 nodes and the relu activation**\n",
        " - Add the **BatchNormalization layer**\n",
        " - Output layer with **activation as 'softmax' and number of nodes equal to the number of classes, i.e., 10**\n",
        " -Compile the model with the **loss equal to categorical_crossentropy, optimizer equal to Adam(learning_rate = 0.0005), and metric equal to 'accuracy'**. Do not fit the model here, just return the compiled model.\n",
        "- Call the nn_model_2 function and store the model in a new variable.\n",
        "- Print the summary of the model.\n",
        "- Fit on the train data with a **validation split of 0.2, batch size = 128, verbose = 1, and epochs = 30**. Store the model building history to use later for visualization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-ZjNBmH0XIV"
      },
      "source": [
        "### **Build and train the new ANN model as per the above mentioned architecture**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEPYLFIPnSDP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJYsvjmw0XIX"
      },
      "source": [
        "### **Plot the Training and Validation Accuracies and write down your Observations.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01ig6BrF1KVy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPW1LlD61RDn"
      },
      "source": [
        "**Observations:_______**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kuXx9Bvu00f"
      },
      "source": [
        "## **Predictions on the test data**\n",
        "\n",
        "- Make predictions on the test set using the second model.\n",
        "- Print the obtained results using the classification report and the confusion matrix.\n",
        "- Final observations on the obtained results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbWMEtTj5Ad0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3li8Ib08yts"
      },
      "source": [
        "**Note:** Earlier, we noticed that each entry of the target variable is a one-hot encoded vector but to print the classification report and confusion matrix, we must convert each entry of y_test to a single label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NByu7uAQ8x9P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_SIoopr0XIg"
      },
      "source": [
        "### **Print the classification report and the confusion matrix for the test predictions. Write your observations on the final results.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRddeJ-3EHT1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjErl4GA2u9s"
      },
      "source": [
        "**Final Observations:__________**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkR4JioMsuIV"
      },
      "source": [
        "## **Using Convolutional Neural Networks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YN2YgkGL_6xQ"
      },
      "source": [
        "### **Load the dataset again and split the data into the train and the test dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqM204HbAjP2",
        "scrolled": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fPqF_xGAjQB"
      },
      "source": [
        "Check the number of images in the training and the testing dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTLJZWjPAjQB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qyMiP_rAjQB"
      },
      "source": [
        "**Observation:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJndFfEVAjQG"
      },
      "source": [
        "## **Data preparation**\n",
        "\n",
        "- Print the shape and the array of pixels for the first image in the training dataset.\n",
        "- Reshape the train and the test dataset because we always have to give a 4D array as input to CNNs.\n",
        "- Normalize the train and the test dataset by dividing by 255.\n",
        "- Print the new shapes of the train and the test dataset.\n",
        "- One-hot encode the target variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4uXqKz1AjQG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "at30iiX1__7F"
      },
      "source": [
        "Reshape the dataset to be able to pass them to CNNs. Remember that we always have to give a 4D array as input to CNNs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9YPwf9ysqWU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODYnoLfaAEGx"
      },
      "source": [
        "Normalize inputs from 0-255 to 0-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOGLAn40AjQG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cS9T4HqjAoyM"
      },
      "source": [
        "Print New shape of Training and Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qf8S5NQAjQG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10QaOV-xR7Jn"
      },
      "source": [
        "### **One-hot encode the labels in the target variable y_train and y_test.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KHWFWKMAjQH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-8jYVQTAjQH"
      },
      "source": [
        "**Observation:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vjx_LI4_AjQH"
      },
      "source": [
        "## **Model Building**\n",
        "\n",
        "Now that we have done data preprocessing, let's build a CNN model.\n",
        "Fix the seed for random number generators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZY5pyF4-KDNt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JUAczhzAjQH"
      },
      "source": [
        "### **Model Architecture**\n",
        "- **Write a function** that returns a sequential model with the following architecture:\n",
        " - First Convolutional layer with **16 filters and the kernel size of 3x3**. Use the **'same' padding** and provide the **input shape = (32, 32, 1)**\n",
        " - Add a **LeakyRelu layer** with the **slope equal to 0.1**\n",
        " - Second Convolutional layer with **32 filters and the kernel size of 3x3 with 'same' padding**\n",
        " - Another **LeakyRelu** with the **slope equal to 0.1**\n",
        " - A **max-pooling layer** with a **pool size of 2x2**\n",
        " - **Flatten** the output from the previous layer\n",
        " - Add a **dense layer with 32 nodes**\n",
        " - Add a **LeakyRelu layer with the slope equal to 0.1**\n",
        " - Add the final **output layer with nodes equal to the number of classes, i.e., 10** and **'softmax' as the activation function**\n",
        " - Compile the model with the **loss equal to categorical_crossentropy, optimizer equal to Adam(learning_rate = 0.001), and metric equal to 'accuracy'**. Do not fit the model here, just return the compiled model.\n",
        "- Call the function cnn_model_1 and store the output in a new variable.\n",
        "- Print the summary of the model.\n",
        "- Fit the model on the training data with a **validation split of 0.2, batch size = 32, verbose = 1, and epochs = 20**. Store the model building history to use later for visualization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWsAd45JKDNu"
      },
      "source": [
        "### **Build and train a CNN model as per the above mentioned architecture.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1jOYANWAjQH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPzfIf9kKDNw"
      },
      "source": [
        "### **Plot the Training and Validation Accuracies and Write your observations.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07oUCr1kAjQH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6zTLyp9AjQH"
      },
      "source": [
        "**Observations:__________**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ukvtg2eMAjQH"
      },
      "source": [
        "Let's build another model and see if we can get a better model with generalized performance.\n",
        "\n",
        "First, we need to clear the previous model's history from the Keras backend. Also, let's fix the seed again after clearing the backend."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbKi93HTolGW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ep19Jd8HAjQH"
      },
      "source": [
        "### **Second Model Architecture**\n",
        "\n",
        "- Write a function that returns a sequential model with the following architecture:\n",
        " - First Convolutional layer with **16 filters and the kernel size of 3x3**. Use the **'same' padding** and provide the **input shape = (32, 32, 1)**\n",
        " - Add a **LeakyRelu layer** with the **slope equal to 0.1**\n",
        " - Second Convolutional layer with **32 filters and the kernel size of 3x3 with 'same' padding**\n",
        " - Add **LeakyRelu** with the **slope equal to 0.1**\n",
        " - Add a **max-pooling layer** with a **pool size of 2x2**\n",
        " - Add a **BatchNormalization layer**\n",
        " - Third Convolutional layer with **32 filters and the kernel size of 3x3 with 'same' padding**\n",
        " - Add a **LeakyRelu layer with the slope equal to 0.1**\n",
        " - Fourth Convolutional layer **64 filters and the kernel size of 3x3 with 'same' padding**\n",
        " - Add a **LeakyRelu layer with the slope equal to 0.1**\n",
        " - Add a **max-pooling layer** with a **pool size of 2x2**\n",
        " - Add a **BatchNormalization layer**\n",
        " - **Flatten** the output from the previous layer\n",
        " - Add a **dense layer with 32 nodes**\n",
        " - Add a **LeakyRelu layer with the slope equal to 0.1**\n",
        " - Add a **dropout layer with the rate equal to 0.5**\n",
        " - Add the final **output layer with nodes equal to the number of classes, i.e., 10** and **'softmax' as the activation function**\n",
        " - Compile the model with the **categorical_crossentropy loss, adam optimizers (learning_rate = 0.001), and metric equal to 'accuracy'**. Do not fit the model here, just return the compiled model.\n",
        "- Call the function cnn_model_2 and store the model in a new variable.\n",
        "- Print the summary of the model.\n",
        "- Fit the model on the train data with a **validation split of 0.2, batch size = 128, verbose = 1, and epochs = 30**. Store the model building history to use later for visualization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5IBLS1eKDNy"
      },
      "source": [
        "### **Build and train the second CNN model as per the above mentioned architecture.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wk9sl2UEAjQH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyhUtMy3KDN1"
      },
      "source": [
        "### **Plot the Training and Validation accuracies and write your observations.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVQu7uWiAjQH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qrrt0Ac3AjQH"
      },
      "source": [
        "**Observations:________**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kja4SnOdAjQI"
      },
      "source": [
        "## **Predictions on the test data**\n",
        "\n",
        "- Make predictions on the test set using the second model.\n",
        "- Print the obtained results using the classification report and the confusion matrix.\n",
        "- Final observations on the obtained results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHCRwRbgKDN2"
      },
      "source": [
        "### **Make predictions on the test data using the second model.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1d-VvaLAjQI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrV1tOG0AjQI"
      },
      "source": [
        "**Note:** Earlier, we noticed that each entry of the target variable is a one-hot encoded vector, but to print the classification report and confusion matrix, we must convert each entry of y_test to a single label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUSHU9W0AjQI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVCa-ysWKDN3"
      },
      "source": [
        "### **Write your final observations on the performance of the model on the test data.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOMq2rCJAjQJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNN9v713AjQJ"
      },
      "source": [
        "**Final Observations:_________**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}